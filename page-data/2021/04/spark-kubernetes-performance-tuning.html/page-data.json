{"componentChunkName":"component---src-templates-post-template-tsx","path":"/2021/04/spark-kubernetes-performance-tuning.html","result":{"data":{"markdownRemark":{"id":"ca884f78-ee38-5bc8-b082-c635741c5e49","html":"<h1 id=\"adaptive-query-execution\" style=\"position:relative;\"><a href=\"#adaptive-query-execution\" aria-label=\"adaptive query execution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adaptive Query Execution</h1>\n<p>Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan. AQE is disabled by default. As of Spark 3.0, there are three major features in AQE, including coalescing post-shuffle partitions, converting sort-merge join to broadcast join, and skew join optimization.</p>\n<p>Enable this optimization: <code class=\"language-text\">spark.sql.adaptive.enabled=true</code></p>\n<p>Reference: <a href=\"https://docs.databricks.com/spark/latest/spark-sql/aqe.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://docs.databricks.com/spark/latest/spark-sql/aqe.html</a></p>\n<h1 id=\"kryo-serialization\" style=\"position:relative;\"><a href=\"#kryo-serialization\" aria-label=\"kryo serialization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kryo serialization</h1>\n<p>Serialization plays an important role in the performance of any distributed application. By default, Spark serializes objects using Java’s ObjectOutputStream framework. Spark can also use the Kryo library (version 4) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all Serializable types and requires you to register the classes you’ll use in the program in advance for best performance..</p>\n<p>Enable this optimization: <code class=\"language-text\">spark.serializer=org.apache.spark.serializer.KryoSerializer</code></p>\n<p>Reference: <a href=\"https://spark.apache.org/docs/latest/tuning.html#data-serialization\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://spark.apache.org/docs/latest/tuning.html#data-serialization</a></p>\n<h1 id=\"tuning-java-pointer\" style=\"position:relative;\"><a href=\"#tuning-java-pointer\" aria-label=\"tuning java pointer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Tuning Java Pointer</h1>\n<p>Java pointer: reduce memory consumption is to avoid the Java features that add overhead, such as pointer-based data structures.</p>\n<p>Enable this optimization:</p>\n<ul>\n<li>Avoid nested structures</li>\n<li>Consider using numeric IDs or enumeration objects instead of strings for keys\n<ul>\n<li>Set the JVM flag <code class=\"language-text\">-XX:+UseCompressedOops</code> to make pointers be four bytes instead of eight</li>\n</ul>\n</li>\n</ul>\n<p><code class=\"language-text\">spec.driver.javaOptions and spec.executor.javaOptions</code></p>\n<p>Reference: <a href=\"https://spark.apache.org/docs/latest/tuning.html#tuning-data-structures\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://spark.apache.org/docs/latest/tuning.html#tuning-data-structures</a></p>\n<h1 id=\"ignoring-data-locality-in-spark\" style=\"position:relative;\"><a href=\"#ignoring-data-locality-in-spark\" aria-label=\"ignoring data locality in spark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ignoring Data Locality in Spark</h1>\n<p>Data Locality in Apache Spark avoided the data movement over network in HDFS, so whenever spark connects to sources like HDFS, s3 it captures the locations of files.</p>\n<p>The above approach makes sense when spark cluster is co-located with distributed file system like HDFS. But with S3, reading locations of files is not useful as spark schedule can’t use this information for co-locating the processing. We need to avoid the <code class=\"language-text\">wasting a lot of time</code> initially to figure all block location of remote files in S3.</p>\n<p><figure class=\"md-figure\"><img src=\"https://1.bp.blogspot.com/-DqjllNPwXAs/YHG4KE1w2bI/AAAAAAAB_Hc/-laW2XvRNHgXvgub8XcNgw83tajD2ihlQCLcBGAsYHQ/s0/image-20210221-073612.png\"><figcaption>Ignoring Data Locality in Spark</figcaption></figure></p>\n<p>Enable this optimization: <code class=\"language-text\">spark.sql.sources.ignoreDataLocality.enabled=true</code></p>\n<p>Reference: <a href=\"https://issues.apache.org/jira/browse/SPARK-29189\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://issues.apache.org/jira/browse/SPARK-29189</a></p>\n<h1 id=\"io-with-s3\" style=\"position:relative;\"><a href=\"#io-with-s3\" aria-label=\"io with s3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>I/O with S3</h1>\n<p>It’s longer time to append data to an existing dataset and in particular, all of Spark jobs have finished, but your command has not finished, it is because driver node is moving the output files of tasks from the job temporary directory to the final destination one-by-one, which is slow with cloud storage (e.g. S3).</p>\n<p>Enable this optimization: <code class=\"language-text\">spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2</code></p>\n<p>Reference: <a href=\"https://kb.databricks.com/data/append-slow-with-spark-2.0.0.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://kb.databricks.com/data/append-slow-with-spark-2.0.0.html</a></p>\n<h1 id=\"dynamic-allocation-shuffle-file-tracking\" style=\"position:relative;\"><a href=\"#dynamic-allocation-shuffle-file-tracking\" aria-label=\"dynamic allocation shuffle file tracking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dynamic Allocation Shuffle File Tracking</h1>\n<p>Experimental. Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs.</p>\n<p>Enable this optimization: <code class=\"language-text\">spark.dynamicAllocation.shuffleTracking.enabled=true</code></p>","fields":{"slug":"/2021/04/spark-kubernetes-performance-tuning.html","tagSlugs":["/tag/data-engineer/","/tag/spark/","/tag/apache-spark/"]},"frontmatter":{"date":"2021-04-10T00:00:00.000+07:00","description":"Spark Performance tuning is a process to improve the performance of the Spark, on this post, I will focus on Spark that runing of Kubernetes.","tags":["Data Engineer","Spark","Apache Spark"],"title":"Spark on Kubernetes Performance Tuning","fbCommentUrl":"none"}}},"pageContext":{"slug":"/2021/04/spark-kubernetes-performance-tuning.html"}},"staticQueryHashes":["251939775","2672868365","401334301"]}