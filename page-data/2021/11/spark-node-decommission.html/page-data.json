{"componentChunkName":"component---src-templates-post-template-tsx","path":"/2021/11/spark-node-decommission.html","result":{"data":{"markdownRemark":{"id":"640241f7-67b0-5f20-8036-1dce43e094c9","html":"<h1 id=\"spark-on-kubernetes---better-handling-for-node-shutdown\" style=\"position:relative;\"><a href=\"#spark-on-kubernetes---better-handling-for-node-shutdown\" aria-label=\"spark on kubernetes   better handling for node shutdown permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark on Kubernetes - better handling for node shutdown</h1>\n<h1 id=\"nodes-decommissioning\" style=\"position:relative;\"><a href=\"#nodes-decommissioning\" aria-label=\"nodes decommissioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Nodes Decommissioning</h1>\n<p>Spark 3.1 on the Kubernetes project is now officially declared as production-ready and Generally Available. Spot instances in Kubernetes can cut your bill by up to 70-80% if you are willing to trade in reliability.</p>\n<p>Now, If you use EC2 Spot or GCP Preemptible instances for costs optimization, in such a scenario, these instances can go away at any moment, and you may need to recompute the tasks they executed. On a shared cluster where the jobs with a higher priority can take the resources used by lower priority and already running jobs.</p>\n<p>The new feature - <strong>SPIP: Add better handling for node shutdown</strong> (<a href=\"https://issues.apache.org/jira/browse/SPARK-20624\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SPARK-20624</a>) was implemented to deal with the problem of losing an executor when working with spot nodes - the need to recompute the shuffle or cached data.</p>\n<h1 id=\"before-this-feature\" style=\"position:relative;\"><a href=\"#before-this-feature\" aria-label=\"before this feature permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Before this feature</h1>\n<p>Before this feature, executor pods are lost when a node kill occurs and shuffle files are lost as well. Therefore driver needs to launch new executors, recomputing the tasks.</p>\n<h1 id=\"with-the-feature\" style=\"position:relative;\"><a href=\"#with-the-feature\" aria-label=\"with the feature permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>With the feature</h1>\n<p>When a node kill occurs, the executor on the spot which is going away is blacklisted. The driver will stop scheduling new tasks on it. The task will be marked as failure (not counting to the max retries).</p>\n<p>Shuffle files and cached data are migrated to another executor. We can also config the <code class=\"language-text\">spark.storage.decommission.fallbackStorage.path=s3a://duyet/spark-storage/</code> to S3 during block manager decommissioning. The storage should be managed by TTL or using <code class=\"language-text\">spark.storage.decommission.fallbackStorage.cleanUp=true</code> to clean up its fallback storage data during shutting down.</p>\n<p><figure class=\"md-figure\"><img src=\"https://1.bp.blogspot.com/--34hINH9_uQ/YZqJZ0URtWI/AAAAAAACXDw/yGn6wkjBWaMDfSZL3Hylwz6ILzP4xKDvACLcBGAsYHQ/s0/spark-spot-node-shutdown.png\"><figcaption>Spark Nodes Decommissioning</figcaption></figure></p>\n<h1 id=\"how-to-enable-this\" style=\"position:relative;\"><a href=\"#how-to-enable-this\" aria-label=\"how to enable this permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How to enable this?</h1>\n<p>We need to config the the <strong>Spark configs</strong> to turn it on</p>\n<ul>\n<li><code class=\"language-text\">spark.decommission.enabled</code>: When decommission enabled, Spark will try its best to shutdown the executor gracefully.</li>\n<li><code class=\"language-text\">spark.storage.decommission.rddBlocks.enabled</code>: Whether to transfer RDD blocks during block manager decommissioning</li>\n<li><code class=\"language-text\">spark.storage.decommission.shuffleBlocks.enabled</code>: Whether to transfer shuffle blocks during block manager decommissioning. Requires a migratable shuffle resolver (like sort based shuffle).</li>\n<li><code class=\"language-text\">spark.storage.decommission.enabled</code>: Whether to decommission the block manager when decommissioning executor.</li>\n<li><code class=\"language-text\">spark.storage.decommission.fallbackStorage.path</code>: The location for fallback storage during block manager decommissioning. For example, <code class=\"language-text\">s3a://spark-storage/</code>. In case of empty, fallback storage is disabled. The storage should be managed by TTL because Spark will not clean it up.</li>\n</ul>\n<p>Please referring to the <a href=\"https://github.com/apache/spark/blob/2e31e2c5f30742c312767f26b17396c4ecfbef72/core/src/main/scala/org/apache/spark/internal/config/package.scala#L1954\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">source code</a> to look at the other available configurations.</p>\n<p>Install <strong>Node Termination Event Handler</strong> to the Kubernetes Cluster. Please looking into projects for:</p>\n<ul>\n<li>AWS: <a href=\"https://github.com/aws/aws-node-termination-handler\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/aws/aws-node-termination-handler</a></li>\n<li>GCP: <a href=\"https://github.com/GoogleCloudPlatform/k8s-node-termination-handler\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/GoogleCloudPlatform/k8s-node-termination-handler</a></li>\n<li>Azure: <a href=\"https://github.com/diseq/k8s-azspot-termination-handler\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/diseq/k8s-azspot-termination-handler</a></li>\n</ul>\n<p>These projects provide an adapter for translating node termination events to graceful pod terminations in Kubernetes so that Spark Decommission can handle them.</p>\n<h1 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h1>\n<ul>\n<li><a href=\"https://www.waitingforcode.com/apache-spark/what-new-apache-spark-3.1-nodes-decommissioning/read\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.waitingforcode.com/apache-spark/what-new-apache-spark-3.1-nodes-decommissioning/read</a></li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-20624\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://issues.apache.org/jira/browse/SPARK-20624</a></li>\n</ul>","fields":{"slug":"/2021/11/spark-node-decommission.html","tagSlugs":["/tag/data-engineer/","/tag/spark/","/tag/kubernetes/"]},"frontmatter":{"date":"2021-11-22T00:00:00.000+07:00","description":"Spark 3.1 on the Kubernetes project is now officially declared as production-ready and Generally Available. Spot instances in Kubernetes can cut your bill by up to 70-80% if you are willing to trade in reliability. The new feature - SPIP: Add better handling for node shutdown (SPARK-20624) was implemented to deal with the problem of losing an executor when working with spot nodes - the need to recompute the shuffle or cached data.\n","tags":["Data Engineer","Spark","Kubernetes"],"title":"Spark on Kubernetes - better handling for node shutdown","fbCommentUrl":"none"}}},"pageContext":{"slug":"/2021/11/spark-node-decommission.html"}},"staticQueryHashes":["251939775","2672868365","401334301"]}