{"componentChunkName":"component---src-templates-post-template-js","path":"/2016/09/spark-convert-text-csv-to-parquet.html","result":{"data":{"markdownRemark":{"id":"3eec045b-84a8-58f6-acb8-4383ca1be767","html":"<p>Lưu trữ dữ liệu dưới dạng <strong>Columnar</strong> như <strong>Apache Parquet</strong> [1] (<strong><a href=\"https://parquet.apache.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://parquet.apache.org</a></strong>) góp phần tăng hiệu năng truy xuất trên Spark lên rất nhiều lần. Bởi vì nó có thể tính toán và chỉ lấy ra 1 phần dữ liệu cần thiết (như 1 vài cột trên CSV), mà không cần phải đụng tới các phần khác của data row. Ngoài ra Parquet còn hỗ trợ flexible compression do đó tiết kiệm được rất nhiều không gian HDFS.  </p>\n<p><a href=\"http://saveto.co/O9kwvB\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><img src=\"https://2.bp.blogspot.com/-e_wBjtB6Fl0/V-ID3ys6F9I/AAAAAAAAd_k/jRxF8H344KM_ywgsxVfQAPy3GDXAd1_fQCK4B/s1600/parquet-logo.png\"></a></p>\n<p>Nếu bạn chứa dữ liệu dạng text trên HDFS và dùng Spark SQL để xử lý, một biện pháp tối ưu bạn nên thử là <strong>chuyển đổi text đó sang Parquet</strong>, tăng tốc độ truy xuất và tối ưu bộ nhớ.  </p>\n<p>Theo một bài viết của <strong>IBM</strong>[2], chuyển đổi sang Parquet giúp tăng tốc độ truy xuất lên <strong>30 lần</strong> (hoặc hơn) tùy trường hợp, bộ nhớ tiết kiệm đến <strong>75%</strong>!  </p>\n<h2 id=\"lets-convert-to-parquet\" style=\"position:relative;\"><a href=\"#lets-convert-to-parquet\" aria-label=\"lets convert to parquet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Let’s convert to Parquet!</h2>\n<p>Spark SQL hỗ trợ đọc và ghi Parquet files, và giữ nguyên được meta data. Parquet schema cho phép data files “self-explanatory” to the Spark SQL applications.  </p>\n<p>Đoạn chương trình sau sử dụng databricks.csv để đọc flat file, sau đó lưu lại dạng Parquet kèm Schema.  </p>\n<p>Đoạn mã trên tự động convert tất cả các file hadoopdsPath+“/catalog_page/* và lưu Parquet vào thư mục <code class=\"language-text\">/user/spark/data/parquet/</code>, mặc định Spark sử dụng chuẩn nén <code class=\"language-text\">gzip</code>, bạn có thể sử dụng compression codec <code class=\"language-text\">uncompressed</code>, <code class=\"language-text\">snappy</code>, hoặc <code class=\"language-text\">lzo</code>.  </p>\n<h2 id=\"convert-1tb-mất-bao-lâu\" style=\"position:relative;\"><a href=\"#convert-1tb-m%E1%BA%A5t-bao-l%C3%A2u\" aria-label=\"convert 1tb mất bao lâu permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Convert 1TB mất bao lâu?</h2>\n<p>Mất <strong>50 phút</strong>, tức khoảng <strong>20GB/phút khi sử dụng 6-datanode Spark 1.5.1</strong>. Tổng lượng bộ nhớ sử dụng là 500GB. Kết quả Parquet files trên HDFS có dạng:  </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">hdfs:///user/spark/data/parquet1000g/catalog_page/_SUCCESS\nhdfs:///user/spark/data/parquet1000g/catalog_page/_common_metadata\nhdfs:///user/spark/data/parquet1000g/catalog_page/_metadata\nhdfs:///user/spark/data/parquet1000g/catalog_page/part-r-00000-a9341639-a804-45bd-b594-8e58220190f4.gz.parquet\nhdfs:///user/spark/data/parquet1000g/catalog_page/part-r-00001-a9341639-a804-45bd-b594-8e58220190f4.gz.parquet</code></pre></div>\n<p>Bộ nhớ tiết kiệm được  </p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ hadoop fs -du -h -s /user/spark/data/text1000g897.9 G  /user/spark/data/text1000g\n$ hadoop fs -du -h -s /user/spark/data/parquet1000g231.4 G  /user/spark/data/parquet1000g</code></pre></div>\n<p>Từ <strong>897.9GB</strong> text, với Parquet chỉ còn lại <strong>231.4GB,</strong> tiết kiệm được khoảng 75%.  </p>\n<h2 id=\"tham-khảo\" style=\"position:relative;\"><a href=\"#tham-kh%E1%BA%A3o\" aria-label=\"tham khảo permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Tham khảo</h2>\n<ul>\n<li>[1] Apache Parquet - <a href=\"https://parquet.apache.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://parquet.apache.org/</a></li>\n<li>[2] <a href=\"https://developer.ibm.com/hadoop/2015/12/03/parquet-for-spark-sql/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">How-to: Convert Text to Parquet in Spark to Boost Performance</a></li>\n<li>[3] <a href=\"https://developer.ibm.com/hadoop/2016/08/11/performance-impact-of-accessing-timestamp-fields-from-big-sql-with-parquet-mr-files/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Performance impact of accessing TIMESTAMP fields from Big SQL with Parquet MR files</a></li>\n</ul>","fields":{"slug":"/2016/09/spark-convert-text-csv-to-parquet.html","tagSlugs":["/tag/data-engineer/","/tag/apache-spark/","/tag/python/","/tag/javascript/","/tag/big-data/","/tag/big-data/","/tag/apache-parquet/","/tag/spark-sql/"]},"frontmatter":{"date":"2016-09-21T10:54:00.000+07:00","description":"Lưu trữ dữ liệu dưới dạng Columnar như Apache Parquet góp phần tăng hiệu năng truy xuất trên Spark lên rất nhiều lần. Bởi vì nó có thể tính toán và chỉ lấy ra 1 phần dữ liệu cần thiết (như 1 vài cột trên CSV), mà không cần phải đụng tới các phần khác của data row. Ngoài ra Parquet còn hỗ trợ flexible compression do đó tiết kiệm được rất nhiều không gian HDFS.","tags":["Data Engineer","Apache Spark","Python","javascript","BigData","Big Data","Apache Parquet","Spark SQL"],"title":"Spark: Convert Text (CSV) to Parquet để tối ưu hóa Spark SQL và HDFS","fbCommentUrl":"none"}}},"pageContext":{"slug":"/2016/09/spark-convert-text-csv-to-parquet.html"}}}