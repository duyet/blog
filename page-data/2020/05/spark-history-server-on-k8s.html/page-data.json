{"componentChunkName":"component---src-templates-post-template-tsx","path":"/2020/05/spark-history-server-on-k8s.html","result":{"data":{"markdownRemark":{"id":"e3355eb0-6518-587a-9f4a-8120d71b0d3e","html":"<p>The problem with running Spark on Kubernetes is the logs go away once the job completes. Spark has tool called the Spark History Server that provides a UI for your past Spark jobs. When you running <a href=\"/2020/05/spark-on-k8s.html\">Spark with Livy on Kubernetes</a> or direct <code class=\"language-text\">spark-submit</code>, <code class=\"language-text\">Spark Driver</code> keeps event logs while running, but after a Spark application is finished <code class=\"language-text\">Spark Driver</code> exits, so these are lost unless you enable event logging and set a folder where the logs are placed. One option is to start <code class=\"language-text\">Spark History Server</code>, and point it to the same log directory so youâ€™ll be able to reach your application logs post-execution</p>\n<p>In this post, I will show you how to use Spark History Server on Kubernetes.</p>\n<h1 id=\"1-enabling-spark-submit-to-log-events\" style=\"position:relative;\"><a href=\"#1-enabling-spark-submit-to-log-events\" aria-label=\"1 enabling spark submit to log events permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Enabling <code class=\"language-text\">spark-submit</code> to log events</h1>\n<p>The history server UI would only show Spark jobs if they are configured to log events to the same location that Spark history server is tracking.</p>\n<blockquote>\n<p>A PVC, HDFS, S3, GCS, WASBS can be used as storage for Spark logs.</p>\n</blockquote>\n<h3 id=\"gcs\" style=\"position:relative;\"><a href=\"#gcs\" aria-label=\"gcs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GCS</h3>\n<p>In the case of GCS, the secret needs to be mounted in the driver and executor pods using the configuration options <code class=\"language-text\">spark.kubernetes.driver.secrets.[SecretName]</code> and <code class=\"language-text\">spark.kubernetes.executor.secrets.[SecretName]</code>. A sample command is given below:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">bin/spark-submit <span class=\"token punctuation\">\\</span>\n    --master k8s://https://<span class=\"token operator\">&lt;</span>k8s-master-url<span class=\"token operator\">></span> <span class=\"token punctuation\">\\</span>\n    --deploy-mode cluster <span class=\"token punctuation\">\\</span>\n    --name spark-pi <span class=\"token punctuation\">\\</span>\n    --class org.apache.spark.examples.SparkPi <span class=\"token punctuation\">\\</span>\n    --conf spark.eventLog.enabled<span class=\"token operator\">=</span>true <span class=\"token punctuation\">\\</span>\n    --conf spark.eventLog.dir<span class=\"token operator\">=</span>gs://spark-hs/ <span class=\"token punctuation\">\\</span>\n    --conf spark.executor.instances<span class=\"token operator\">=</span><span class=\"token number\">1</span> <span class=\"token punctuation\">\\</span>\n    --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile<span class=\"token operator\">=</span>/etc/secrets/sparkonk8s.json <span class=\"token punctuation\">\\</span>\n    --conf spark.kubernetes.driver.secrets.history-secrets<span class=\"token operator\">=</span>/etc/secrets <span class=\"token punctuation\">\\</span>\n    --conf spark.kubernetes.executor.secrets.history-secrets<span class=\"token operator\">=</span>/etc/secrets <span class=\"token punctuation\">\\</span>\n    --conf spark.kubernetes.container.image<span class=\"token operator\">=</span>lightbend/spark-history-server:2.4.0 <span class=\"token punctuation\">\\</span>\n    local:///opt/spark/examples/jars/spark-examples_2.11-2.4.0.jar</code></pre></div>\n<h3 id=\"s3\" style=\"position:relative;\"><a href=\"#s3\" aria-label=\"s3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3</h3>\n<p>In the case of S3, it is recommended to use IAM based authentication. The IAM role should have equivalent access to AmazonS3FullAccess. To write event logs to S3, you need to provide configs as below:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">in/spark-submit <span class=\"token punctuation\">\\</span>\n    --master k8s://https://<span class=\"token operator\">&lt;</span>k8s-master-url<span class=\"token operator\">></span> <span class=\"token punctuation\">\\</span>\n    --deploy-mode cluster <span class=\"token punctuation\">\\</span>\n    --name spark-pi <span class=\"token punctuation\">\\</span>\n    --class org.apache.spark.examples.SparkPi <span class=\"token punctuation\">\\</span>\n    --conf spark.eventLog.enabled<span class=\"token operator\">=</span>true <span class=\"token punctuation\">\\</span>\n    --conf spark.eventLog.dir<span class=\"token operator\">=</span>s3a://spark-hs/ <span class=\"token punctuation\">\\</span>\n    --conf spark.kubernetes.container.image<span class=\"token operator\">=</span>lightbend/spark-history-server:2.4.0 <span class=\"token punctuation\">\\</span>\n    local:///opt/spark/examples/jars/spark-examples_2.11-2.4.0.jar</code></pre></div>\n<p>When not using the IAM based authentication, you need to provide additional configs for authentication as below:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">--conf spark.hadoop.fs.s3a.access.key<span class=\"token operator\">=</span>your-AWS-access-key-ID <span class=\"token punctuation\">\\</span>\n--conf spark.hadoop.fs.s3a.secret.key<span class=\"token operator\">=</span>your-AWS-secret-access-key</code></pre></div>\n<h3 id=\"hdfs\" style=\"position:relative;\"><a href=\"#hdfs\" aria-label=\"hdfs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HDFS</h3>\n<p>In the case of HDFS, only two flags are required:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">--conf spark.eventLog.enabled<span class=\"token operator\">=</span>true\n--conf spark.eventLog.dir<span class=\"token operator\">=</span>hdfs://hdfs/history/</code></pre></div>\n<h1 id=\"2-installing-the-spark-history-server-chart\" style=\"position:relative;\"><a href=\"#2-installing-the-spark-history-server-chart\" aria-label=\"2 installing the spark history server chart permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Installing the Spark History Server Chart</h1>\n<p>To install the chart with S3 support:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ helm repo <span class=\"token function\">add</span> stable https://kubernetes-charts.storage.googleapis.com\n$ helm <span class=\"token function\">install</span> -f values.yaml stable/spark-history-server --namespace spark-history-server</code></pre></div>\n<p>With <code class=\"language-text\">values.yaml</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">s3</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">enableS3</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token key atrule\">enableIAM</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token key atrule\">logDirectory</span><span class=\"token punctuation\">:</span> s3a<span class=\"token punctuation\">:</span>//spark<span class=\"token punctuation\">-</span>hs/\n  <span class=\"token comment\"># accessKeyName is an AWS access key ID. Omit for IAM role-based or provider-based authentication.</span>\n  <span class=\"token key atrule\">accessKeyName</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>access<span class=\"token punctuation\">-</span>key\n  <span class=\"token comment\"># secretKey is AWS secret key. Omit for IAM role-based or provider-based authentication.</span>\n  <span class=\"token key atrule\">secretKeyName</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>secret<span class=\"token punctuation\">-</span>key\n\n<span class=\"token key atrule\">gcs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">enableGCS</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n  <span class=\"token key atrule\">logDirectory</span><span class=\"token punctuation\">:</span> gs<span class=\"token punctuation\">:</span>//spark<span class=\"token punctuation\">-</span>hs/\n\n<span class=\"token key atrule\">pvc</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">enablePVC</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n\n<span class=\"token key atrule\">nfs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">enableExampleNFS</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n\n<span class=\"token key atrule\">service</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> NodePort\n  <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">18080</span>\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span></code></pre></div>\n<p>For details about installing the chart to use others storage, see configurations options <a href=\"https://github.com/helm/charts/tree/master/stable/spark-history-server#configurations\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>.</p>\n<p><strong>Viewing the UI:</strong> After the chart is successfully installed, a message would be printed out to the console with details about how to access the UI. Depending on what <code class=\"language-text\">service.type</code> is specified, different instructions would be presented on the console. Valid <code class=\"language-text\">service.type</code> values are <code class=\"language-text\">LoadBalancer</code>, <code class=\"language-text\">NodePort</code> and <code class=\"language-text\">ClusterIP</code>.</p>\n<p>From above config, you should able to access the UI via: <code class=\"language-text\">http://[nodeIp]:18080</code></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1327ae27dcead4e25b2900c86ac415b1/f3c07/spark-history-server.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAACpElEQVQoz6WTTU8TQRjH9+DBL2E0NXokcJCLcPCgRAuGhBtFsyEhNag3jB/AD4EHj+LBeDDxoKXEJrX0hQpCS1DE0pedfe3utvva1+njMyPq1cRJfpnN7Dy//T+TWWH21mTszt3by7PRWXFqakqcnLwmXr9xU1xcuifGYotIjLOwsCDOzc2J8/PzfN/Y2Jg4Pj4uTkxMiNFodHl6enpREITzwpvXryD5/h18SCRgY2ODk9hMwk6xCLlcDvL5PBQKBchms5DJZGB7extSqRQkk0nO1tYWX9/c3KQzMzOXBA9H6HvQ6XYHYRgOu93ukA0A+FcG8Gu0V1ZWIoJlmYEkSSATQlVFGQVBMEIfBzeNKKV8ZrDxe2bvz2bKbFjniqIYEWRFCWy7Ba7nj3q9PlCswaTgOA6HhcUmOL1eD1zXBcuyOLZtQ6fToY7rsXUvHo9HhKpmBUeaC98Nb/RVc+Cb4UFdVqFRr0O1WgWWvo7PtVoNZFn+I2o2m3BaqUCVyPTlfgWMVttbe7QaESTLDSp2CCemPzpu+nBqd0A3scAwQNM0jq7roKoql7D0DJZUUxSoqzpd3z0Fo+16Tx4+iAjEcoLqmfAEhbVWCE3LBsPQQcECJjRQTgjhQjwrDjsORSYo1OjzvcpfoWS2/RMzgIrpD48Nj6KcmpZFUURRSLFNiuloo9GgmJSijPq+T9vtNpUliVaJMlzf/QF6y3HWmFB3wkHDG0DN6UHD7YMaDAE/Bq1WiycyTZOfG2sbJdDv92EwGPC2m7hGDBNelBpg+2H36ePVy0Jmr3ycLB6qiZ2SlCiUSPLzIcnvFEkumyV4iTl4wUk6nSZ4wcnBwQEpl8vky/4+yXxKk1QmKz17+1HbLR0exe8vXcC/RbiEXEWu/Aes/iJy7ida7d+nq8/3pQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/1327ae27dcead4e25b2900c86ac415b1/c85cb/spark-history-server.webp 300w,\n/static/1327ae27dcead4e25b2900c86ac415b1/e88ff/spark-history-server.webp 600w,\n/static/1327ae27dcead4e25b2900c86ac415b1/92f8c/spark-history-server.webp 1200w,\n/static/1327ae27dcead4e25b2900c86ac415b1/62ed8/spark-history-server.webp 1800w,\n/static/1327ae27dcead4e25b2900c86ac415b1/46261/spark-history-server.webp 2400w,\n/static/1327ae27dcead4e25b2900c86ac415b1/15b85/spark-history-server.webp 2516w\"\n              sizes=\"(max-width: 1200px) 100vw, 1200px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/1327ae27dcead4e25b2900c86ac415b1/5a46d/spark-history-server.png 300w,\n/static/1327ae27dcead4e25b2900c86ac415b1/0a47e/spark-history-server.png 600w,\n/static/1327ae27dcead4e25b2900c86ac415b1/c1b63/spark-history-server.png 1200w,\n/static/1327ae27dcead4e25b2900c86ac415b1/d61c2/spark-history-server.png 1800w,\n/static/1327ae27dcead4e25b2900c86ac415b1/97a96/spark-history-server.png 2400w,\n/static/1327ae27dcead4e25b2900c86ac415b1/f3c07/spark-history-server.png 2516w\"\n            sizes=\"(max-width: 1200px) 100vw, 1200px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/1327ae27dcead4e25b2900c86ac415b1/c1b63/spark-history-server.png\"\n            alt=\"spark history server\"\n            title=\"spark history server\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h1 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h1>\n<ul>\n<li><a href=\"https://github.com/helm/charts/tree/master/stable/spark-history-server\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/helm/charts/tree/master/stable/spark-history-server</a></li>\n</ul>","fields":{"slug":"/2020/05/spark-history-server-on-k8s.html","tagSlugs":["/tag/data-engineer/","/tag/spark/","/tag/kubernetes/","/tag/spark-history-server/"]},"frontmatter":{"date":"2020-05-29T11:00:00.000+07:00","description":"The problem with running Spark on Kubernetes is the logs go away once the job completes. Spark has tool called the Spark History Server that provides a UI for your past Spark jobs. In this post, I will show you how to use Spark History Server on Kubernetes.","tags":["Data Engineer","Spark","Kubernetes","Spark History Server"],"title":"Spark History Server on Kubernetes","fbCommentUrl":"none"}}},"pageContext":{"slug":"/2020/05/spark-history-server-on-k8s.html"}},"staticQueryHashes":["251939775","2672868365","401334301"]}