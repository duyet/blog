{"componentChunkName":"component---src-templates-post-template-js","path":"/2020/05/spark-on-k8s.html","result":{"data":{"markdownRemark":{"id":"5b5f5a96-0532-5435-910b-9dc2ae8bd6d3","html":"<p>Most of the big data applications need multiple services likes HDFS, YARN, Spark and their clusters.\nSolutions like EMR, Databricks etc help us to simplify the deployment. But then users will be locked into those specific services. These distributed systems require a cluster-management system to handle tasks such as checking node health and scheduling jobs. With the Apache Spark, you can run it like a scheduler YARN, Mesos, standalone mode or now Kubernetes, which is now experimental.</p>\n<p>There are many ways to deploy Spark Application on Kubernetes:</p>\n<ol>\n<li><code class=\"language-text\">spark-submit</code> directly submit a Spark application to a Kubernetes cluster</li>\n<li>Using <a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Spark Operator</a></li>\n<li>Using Livy to Submit Spark Jobs on Kubernetes</li>\n</ol>\n<h1 id=\"yarn-pain-points\" style=\"position:relative;\"><a href=\"#yarn-pain-points\" aria-label=\"yarn pain points permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>YARN pain points</h1>\n<ul>\n<li>Management is difficult</li>\n<li>Complicated OSS software stack: version and dependency management is hard.</li>\n<li>Isolation is hard</li>\n</ul>\n<h1 id=\"why-spark-on-kubernetes\" style=\"position:relative;\"><a href=\"#why-spark-on-kubernetes\" aria-label=\"why spark on kubernetes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why Spark on Kubernetes</h1>\n<p>Spark can run on clusters managed by Kubernetes. This feature makes use of native Kubernetes scheduler that has been added to Spark. Kubernetes offers some powerful benefits as a resource manager for Big Data applications, but comes with its own complexities.</p>\n<p>It is using custom resource definitions and operators as a means to extend the Kubernetes API. So far, it has open-sourced operators for Spark and Apache Flink, and is working on more.</p>\n<p>Here are three primary benefits to using Kubernetes as a resource manager:</p>\n<ul>\n<li>Unified management — Getting away from two cluster management interfaces if your organization already is using Kubernetes elsewhere.</li>\n<li>Ability to isolate jobs — You can move models and ETL pipelines from dev to production without the headaches of dependency management.</li>\n<li>Resilient infrastructure — You don’t worry about sizing and building the cluster, manipulating Docker files or Kubernetes networking configurations.</li>\n</ul>\n<h2 id=\"1-spark-submit-directly-submit-a-spark-application-to-a-kubernetes-cluster\" style=\"position:relative;\"><a href=\"#1-spark-submit-directly-submit-a-spark-application-to-a-kubernetes-cluster\" aria-label=\"1 spark submit directly submit a spark application to a kubernetes cluster permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. <code class=\"language-text\">spark-submit</code> directly submit a Spark application to a Kubernetes cluster</h2>\n<p><code class=\"language-text\">spark-submit</code> can be directly used to submit a Spark application to a Kubernetes cluster. The submission mechanism works as follows:</p>\n<ul>\n<li>Spark creates a <em>Spark driver</em> running within a <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kubernetes pod</a>.</li>\n<li>The driver creates executors which are also running within Kubernetes pods and connects to them, and executes application code.</li>\n<li>When the application completes, the executor pods terminate and are cleaned up, but the driver pod persists logs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 761px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fc4a885211a5929b3f133a7fc7314789/8c857/k8s-cluster-mode.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC4klEQVQoz32S2U8aYRTF/d/6Ykxta2sbTWNrClYrpoI7MkgliqZWqY2KGOOCLDPsIC0V2USWgYEZViUq4laXxNamkQiDw3QgtKYv/XJzcx7uL+feL6eC/P8rFKiWz+HbCLoXie9FYrvh6C4WTYYwPJerKM+QxaHQ933oAHYkfc4tnysJW7e9uoOA/iioO0SgHbcsZpPH7IqEE9p161NwBs/+gUsOi6lNkWnuTC3/pgVPFMvGER7Ts8AKrrAQSScqoxuF9St8unmSGZICYeWP3PU/8JcdF9LVmmpvSTKbEXaXYZDDQxRARMXFoHcRdZt/ocrM54VADgYJYvqrXKbi720ESc45UE/Dg7266u2H9+DmRoOABwRknLCyHwV5EXXt+vvWjanXmzN9qGI4pivDBFGkbQmyQYwF3jwN11ahtZVeBn2VDwB+WX9YSVkNREC6xfBcZe8NydgoOBTV3TkTROGDmZhyOaO0J0f1Neln9xHai9mliT4M4pSqy6fv9pqYLnNfQMvG5EPR0tpE8VjSuZ3XIYW1Y9+4ZlKs+jSrmvy4KmKjEDeiKpJeIxvR9cB6Dgb2wkY2prhzzt+SY6aby1+k7tjfjinYCU1PXM2OqakN22EJa+MrB1O1OJUdHj0vpmh2qFketSCuLcOaAD69nqUEmIa7kZWBEASEQAAFH62N1FvGAURDc09Xrg72B6RN7tnqzwKme4kf1ZRh0Jc/vCx+mjINdwWlAKbkhpWUqFkfbfOI3wYW6pzCJtcUwzf/2DbW6hHR3TP8aMk5m80S+ZtcLksSpObQ3xOSDYRVFAygUItlmmEVcVEl0zVPMwl7vZJuWEI3CVnO+eGI9grPVGQymdT+/nUmQwVUnvYw/IvdQVlnUNoRkDQZJl7pxll+SZtd/BIabbWKWPAyTfuhUT9GxaaYMBzHqXjhebxwS2z9PLGcRh1ncXupbKexYlH6PGG7SFDdfl7U1vO46yJ5c4v/BjcnbUwgcshSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/fc4a885211a5929b3f133a7fc7314789/c85cb/k8s-cluster-mode.webp 300w,\n/static/fc4a885211a5929b3f133a7fc7314789/e88ff/k8s-cluster-mode.webp 600w,\n/static/fc4a885211a5929b3f133a7fc7314789/6e34c/k8s-cluster-mode.webp 761w\"\n          sizes=\"(max-width: 761px) 100vw, 761px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/fc4a885211a5929b3f133a7fc7314789/5a46d/k8s-cluster-mode.png 300w,\n/static/fc4a885211a5929b3f133a7fc7314789/0a47e/k8s-cluster-mode.png 600w,\n/static/fc4a885211a5929b3f133a7fc7314789/8c857/k8s-cluster-mode.png 761w\"\n          sizes=\"(max-width: 761px) 100vw, 761px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/fc4a885211a5929b3f133a7fc7314789/8c857/k8s-cluster-mode.png\"\n          alt=\"k8s cluster mode\"\n          title=\"k8s cluster mode\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n  </a>\n    </span></p>\n<p>For example to launch Spark Pi in cluster mode:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ bin/spark-submit <span class=\"token punctuation\">\\</span>\n    --master k8s://https://<span class=\"token operator\">&lt;</span>k8s-apiserver-host<span class=\"token operator\">></span>:<span class=\"token operator\">&lt;</span>k8s-apiserver-port<span class=\"token operator\">></span> <span class=\"token punctuation\">\\</span>\n    --deploy-mode cluster <span class=\"token punctuation\">\\</span>\n    --name spark-pi <span class=\"token punctuation\">\\</span>\n    --class org.apache.spark.examples.SparkPi <span class=\"token punctuation\">\\</span>\n    --conf spark.executor.instances<span class=\"token operator\">=</span><span class=\"token number\">5</span> <span class=\"token punctuation\">\\</span>\n    --conf spark.kubernetes.container.image<span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>spark-image<span class=\"token operator\">></span> <span class=\"token punctuation\">\\</span>\n    local:///path/to/examples.jar</code></pre></div>\n<p>More detail at: <a href=\"https://spark.apache.org/docs/latest/running-on-kubernetes.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p>\n<h2 id=\"2-using-spark-operator\" style=\"position:relative;\"><a href=\"#2-using-spark-operator\" aria-label=\"2 using spark operator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Using Spark Operator</h2>\n<p>The <a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kubernetes Operator</a> for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources for specifying, running, and surfacing status of Spark applications. </p>\n<p>The easiest way to install the <a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kubernetes Operator</a> for Apache Spark is to use the Helm chart.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ helm repo <span class=\"token function\">add</span> incubator http://storage.googleapis.com/kubernetes-charts-incubator\n$ helm <span class=\"token function\">install</span> incubator/sparkoperator --namespace spark-operator</code></pre></div>\n<p>This will install the Kubernetes Operator for Apache Spark into the namespace <code class=\"language-text\">spark-operator</code>. The operator by default watches and handles SparkApplications in every namespaces.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2ac499c6725d8f7dad1c01e210a1d921/d9199/spark-operator-architecture-diagram.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB4klEQVQoz22Sy27TQBSG/Wp9jvIECMSKqF2xYFMkJCTKAioWoaAIgbpBVESqlFbBRECSthERlzaOkji+JR7fb3PmYmacpizKkRfjc+af75/fVjgvWfXw8qrE+mSQ6sbCcSzTtIIgZIwTQhjnjBCaphRjuaZUWUtKMf16PD0+1JZ2XHLAGCglVzMuECUsl26jMa/XjV5P9HBRKJoNrWF6MoxRiM86c/Vo4qOUVLLQy6Yjdzb2dA3Rsgza7dHGxo/NzVGrJabieOXbJd5vxw01cQJaMSQpCNOp5X9Wp80PWv/LvK/qSQHBYKBvbeu7u7HjSKcASl6A5XjIT1h1dUKl+ujUv/18Vts3njW9a+OMs4Ixyvn6mkQxUd7/jS5mIQbZZUwOPnb9O3tGrW4+fGuJhjkcf3/TjJAvNxDCJYcDgX+BXUct6lPPv/fS2HptPjqwWckjGwWXlkho5/xFR++XRHwamb/CK0t8bWYlPuz69wX2nb1z4AgM5dTCiHB6jn7Z4YIRasR2mEc3yJXt96p768nk7t6s9soQrxfBZLvz2It9QYQC3AA96D49c35KMgVhIWcVfGXAQtmpFg/GyR89FalmRbaI3TiKkiwRWwHATb0CF1IMOQZIGC//W/LfWvm5UX8BHvNdfsrU7yYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/2ac499c6725d8f7dad1c01e210a1d921/c85cb/spark-operator-architecture-diagram.webp 300w,\n/static/2ac499c6725d8f7dad1c01e210a1d921/e88ff/spark-operator-architecture-diagram.webp 600w,\n/static/2ac499c6725d8f7dad1c01e210a1d921/e46b2/spark-operator-architecture-diagram.webp 960w\"\n          sizes=\"(max-width: 960px) 100vw, 960px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/2ac499c6725d8f7dad1c01e210a1d921/5a46d/spark-operator-architecture-diagram.png 300w,\n/static/2ac499c6725d8f7dad1c01e210a1d921/0a47e/spark-operator-architecture-diagram.png 600w,\n/static/2ac499c6725d8f7dad1c01e210a1d921/d9199/spark-operator-architecture-diagram.png 960w\"\n          sizes=\"(max-width: 960px) 100vw, 960px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/2ac499c6725d8f7dad1c01e210a1d921/d9199/spark-operator-architecture-diagram.png\"\n          alt=\"spark operator architecture diagram\"\n          title=\"spark operator architecture diagram\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n  </a>\n    </span></p>\n<p>To config the Spark Application, we defines an YAML file:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token comment\"># spark-pi.yaml</span>\n\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"sparkoperator.k8s.io/v1beta2\"</span>\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> SparkApplication\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> spark<span class=\"token punctuation\">-</span>pi\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> default\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Scala\n  <span class=\"token key atrule\">mode</span><span class=\"token punctuation\">:</span> cluster\n  <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"gcr.io/spark-operator/spark:v2.4.5\"</span>\n  <span class=\"token key atrule\">imagePullPolicy</span><span class=\"token punctuation\">:</span> Always\n  <span class=\"token key atrule\">mainClass</span><span class=\"token punctuation\">:</span> org.apache.spark.examples.SparkPi\n  <span class=\"token key atrule\">mainApplicationFile</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"local:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar\"</span>\n  <span class=\"token key atrule\">sparkVersion</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"2.4.5\"</span>\n  <span class=\"token key atrule\">restartPolicy</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Never\n\n  <span class=\"token key atrule\">driver</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">cores</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">coreLimit</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1200m\"</span>\n    <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"512m\"</span>\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> 2.4.5\n    <span class=\"token key atrule\">serviceAccount</span><span class=\"token punctuation\">:</span> spark\n  <span class=\"token key atrule\">executor</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">cores</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">instances</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"512m\"</span>\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> 2.4.5</code></pre></div>\n<p>Note that <code class=\"language-text\">spark-pi.yaml</code> configures the driver pod to use the spark service account to communicate with the Kubernetes API server. </p>\n<p>To run the Spark Pi example, run the following command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl apply -f spark-pi.yaml</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl get po\nNAME                                                              READY     STATUS    RESTARTS   AGE\nspark-pi-1590286117050-driver                                     <span class=\"token number\">1</span>/1       Running   <span class=\"token number\">0</span>          2m\nspark-pi-1590286117050-exec-1                                     <span class=\"token number\">0</span>/1       Pending   <span class=\"token number\">0</span>          38s</code></pre></div>\n<p>Refs:</p>\n<ul>\n<li><a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/GoogleCloudPlatform/spark-on-k8s-operator</a></li>\n<li><a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/quick-start-guide.md\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Quick Start Guide</a></li>\n</ul>\n<h2 id=\"3-using-livy\" style=\"position:relative;\"><a href=\"#3-using-livy\" aria-label=\"3 using livy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Using Livy</h2>\n<p>Apache Livy is a service that enables easy interaction with a Spark cluster over a REST interface.\nThe cons is that Livy is written for Yarn. But Yarn is just Yet Another resource manager with containers abstraction adaptable to the Kubernetes concepts. Under the hood Livy parses POSTed configs and does <code class=\"language-text\">spark-submit</code> for you, bypassing other defaults configured for the Livy server.</p>\n<p>The high-level architecture of Livy on Kubernetes is the same as for Yarn.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/df48f5c54c028a38957574f7bd9213d6/8b936/livy-spark-k8s.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACk0lEQVQoz11SS2hUMRQNCEURN1Jw4c5Sq2hVRAS3MuAXRBCtdCXiVqy6rhvRrRX/Cir4QaTgQkURquCnit+CtOpoW6ed6djOzPvlJXkveTnevPGHFw435Oace24StrdnA3t783rL56plY5Wwq/9VgMmJsi6NFk1p7FtWLo2bJI7SVESackawintGCW6ViIyS3KZuX3BX16ynZz3rXziP+cVBJqKwu96IoDgd5BG0aIIOIhWc1hyJFOCfH0He64W42wv56goSJUGi+Tl28EChZXDLxj1nGZtjsmyHlhxRGJpECRvE2kZCW5sZK4Sw3PdsopStv+m36thamx5dbdXtHptoY1MeWueetV3qWnDuzLaJ6CpbA2CTSRWMhYkS2FsvOZ6MSOgMqNRClKem4UcSU0OPwM/vQnx6O9TD40jSlByGTYd9c7fOPnJify+7f7H1y8hE92R1BrEyhsTteJVjqi7wO3SiiOhGDyFkDJ9TMxX9EXPXwhYdRuuqy5fusAv72qpfKzsDr+G4ZrCY2EZsc6EPJY1AWmSJBN0zkUPEnARDAe3ulFNWoumwUCis6GhvCxhj64i72RDJD7m5OlC1w9/9fMTT98soVmJqk0BJiRlfQiYmbzYwLHH3PdWsbQouW97Z2rFk6SESnE/17e51/XrNcG/GRl4dUeChNv0DceiTixgq0bj2PMKLrwoJEd59C/Cu6OXiygm2L25nLl4+e8okD3fn30PGhmAJcK+e0Thu3zkwWuPjWB0fJyU8CTymR3swFJPBrOmws3MlO3Wyr4UaMCLtTuKc6D6z+wb4H1o3R/1USXHjBcfw9wBDo42/Dl2Q0CyCy12/nBjK1q3/RS6Y0qDkpubFeD1SJZksh3Pu6j8BXzXqlvGKXdAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/df48f5c54c028a38957574f7bd9213d6/c85cb/livy-spark-k8s.webp 300w,\n/static/df48f5c54c028a38957574f7bd9213d6/e88ff/livy-spark-k8s.webp 600w,\n/static/df48f5c54c028a38957574f7bd9213d6/92f8c/livy-spark-k8s.webp 1200w,\n/static/df48f5c54c028a38957574f7bd9213d6/338ac/livy-spark-k8s.webp 1368w\"\n          sizes=\"(max-width: 1200px) 100vw, 1200px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/df48f5c54c028a38957574f7bd9213d6/5a46d/livy-spark-k8s.png 300w,\n/static/df48f5c54c028a38957574f7bd9213d6/0a47e/livy-spark-k8s.png 600w,\n/static/df48f5c54c028a38957574f7bd9213d6/c1b63/livy-spark-k8s.png 1200w,\n/static/df48f5c54c028a38957574f7bd9213d6/8b936/livy-spark-k8s.png 1368w\"\n          sizes=\"(max-width: 1200px) 100vw, 1200px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/df48f5c54c028a38957574f7bd9213d6/c1b63/livy-spark-k8s.png\"\n          alt=\"livy spark k8s\"\n          title=\"livy spark k8s\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n  </a>\n    </span></p>\n<p>After the job submission Livy discovers Spark Driver Pod scheduled to the Kubernetes cluster with Kubernetes API and starts to track its state, cache Spark Pods logs and details descriptions making that information available through Livy REST API, builds routes to Spark UI, Spark History Server, Monitoring systems with Kubernetes Ingress resources, Nginx Ingress Controller in particular and displays the links on Livy Web UI.</p>\n<p>The basic Spark on Kubernetes setup consists of the only Apache Livy server deployment, which can be installed with the Livy Helm chart.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> jahstreet https://jahstreet.github.io/helm-charts\nkubectl create namespace spark-jobs\nhelm upgrade --install livy --namespace spark-jobs jahstreet/livy</code></pre></div>\n<p>Now when Livy is up and running we can submit Spark job via <a href=\"https://livy.incubator.apache.org/docs/latest/rest-api.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Livy REST API</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token builtin class-name\">exec</span> livy-0 -- <span class=\"token punctuation\">\\</span>\n    <span class=\"token function\">curl</span> -s -k -H <span class=\"token string\">'Content-Type: application/json'</span> -X POST <span class=\"token punctuation\">\\</span>\n      -d <span class=\"token string\">'{\n            \"name\": \"SparkPi-01\",\n            \"className\": \"org.apache.spark.examples.SparkPi\",\n            \"numExecutors\": 5,\n            \"file\": \"local:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar\",\n            \"args\": [\"10000\"],\n            \"conf\": {\n                \"spark.kubernetes.namespace\": \"livy\"\n            }\n          }'</span> <span class=\"token string\">\"http://localhost:8998/batches\"</span> <span class=\"token operator\">|</span> jq</code></pre></div>\n<p>To track the running Spark job we can use all the available Kubernetes tools:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ k get pod\n\nNAME                                          READY   STATUS         RESTARTS   AGE\nlivy-0                                        <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          3h\nsparkpi01aasdbtyovr-1590286117050-driver      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m\nsparkpi01aasdbtyovr-1590286117050-exec-1      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m\nsparkpi01aasdbtyovr-1590286117050-exec-2      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m\nsparkpi01aasdbtyovr-1590286117050-exec-3      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m\nsparkpi01aasdbtyovr-1590286117050-exec-4      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m\nsparkpi01aasdbtyovr-1590286117050-exec-5      <span class=\"token number\">1</span>/1     Running        <span class=\"token number\">0</span>          1m</code></pre></div>\n<p>or the Livy REST API: </p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">k <span class=\"token builtin class-name\">exec</span> livy-0 -- <span class=\"token function\">curl</span> -s http://localhost:8998/batches/<span class=\"token variable\">$BATCH_ID</span> <span class=\"token operator\">|</span> jq</code></pre></div>\n<p>With Livy, we can easy to integrate with <a href=\"/tag/airflow/\">Apache Airflow</a> to manage Spark Jobs on Kubernetes at scale.</p>\n<p>Refs: <a href=\"https://github.com/jahstreet/spark-on-kubernetes-helm\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/jahstreet/spark-on-kubernetes-helm</a></p>\n<h1 id=\"how-to-choose-between-spark-submit-and-spark-operator\" style=\"position:relative;\"><a href=\"#how-to-choose-between-spark-submit-and-spark-operator\" aria-label=\"how to choose between spark submit and spark operator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How To Choose Between <code class=\"language-text\">spark-submit</code> and Spark Operator?</h1>\n<p>Since <code class=\"language-text\">spark-submit</code> is built into Apache Spark, it’s easy to use and has well-documented configuration options. It is particularly well-suited for submitting Spark jobs in an isolated manner in development or production, and it allows you to build your own tooling around it if that serves your purposes. You could use it to integrate directly with a job flow tool (e.g. <a href=\"https://github.com/apache/airflow/blob/master/airflow/contrib/operators/spark_submit_operator.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Apache AirFlow</a>, Apache Livy). Although easy to use, spark-submit lacks functionalities like supporting basic operations for job management. </p>\n<p>If you want to manage your Spark jobs with one tool in a declarative way with some unique management and monitoring features, the Operator is the best available solution.\nYou can manage your Spark Jobs by <a href=\"https://www.weave.works/technologies/gitops/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">GitOps workflows</a> like the Fluxcd does.\nIt saves you effort in monitoring the status of jobs, looking for logs, and keeping track of job versions. This last point is especially crucial if you have a lot of users and many jobs run in your cluster at any given time.</p>","fields":{"slug":"/2020/05/spark-on-k8s.html","tagSlugs":["/tag/data-engineer/","/tag/spark/","/tag/kubernetes/","/tag/livy/"]},"frontmatter":{"date":"2020-05-24T11:00:00.000+07:00","description":"Spark can run on clusters managed by Kubernetes. This feature makes use of native Kubernetes scheduler that has been added to Spark.","tags":["Data Engineer","Spark","Kubernetes","Livy"],"title":"3 ways to run Spark on Kubernetes","fbCommentUrl":"none"}}},"pageContext":{"slug":"/2020/05/spark-on-k8s.html"}},"staticQueryHashes":["251939775","2672868365","401334301"]}