{"componentChunkName":"component---src-templates-tag-template-tsx","path":"/tag/apache-spark","result":{"data":{"site":{"siteMetadata":{"title":"Tôi là Duyệt","subtitle":"Data Engineer @ Fossil. I blog about web development, machine learning, data engineering and more."}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/2021/04/spark-kubernetes-performance-tuning.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"Spark on Kubernetes Performance Tuning","date":"2021-04-10T00:00:00.000+07:00","category":"Data Engineer","description":"Spark Performance tuning is a process to improve the performance of the Spark, on this post, I will focus on Spark that runing of Kubernetes.","thumbnail":"https://1.bp.blogspot.com/-DqjllNPwXAs/YHG4KE1w2bI/AAAAAAAB_Hc/-laW2XvRNHgXvgub8XcNgw83tajD2ihlQCLcBGAsYHQ/s0/image-20210221-073612.png"}}},{"node":{"fields":{"slug":"/2017/05/cai-apache-spark-standalone-ban-pre.html","categorySlug":"/category/data/"},"frontmatter":{"title":"Cài Apache Spark standalone bản pre-built","date":"2017-05-31T22:19:00.001+07:00","category":"Data","description":"Mình nhận được nhiều phản hồi từ bài viết BigData - Cài đặt Apache Spark trên Ubuntu 14.04 rằng sao cài khó và phức tạp thế. Thực ra bài viết đó mình hướng dẫn cách build và install từ source.","thumbnail":"https://4.bp.blogspot.com/-5hwfzlugnac/WS7b6rg8cQI/AAAAAAAAlDg/Rgpp6oj-lGQludEAlYo9YtOrGCeudR0zgCLcB/s1600/Screenshot%2Bfrom%2B2017-05-31%2B22-02-05.png"}}},{"node":{"fields":{"slug":"/2016/09/spark-convert-text-csv-to-parquet.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"Spark: Convert Text (CSV) to Parquet để tối ưu hóa Spark SQL và HDFS","date":"2016-09-21T10:54:00.000+07:00","category":"Data Engineer","description":"Lưu trữ dữ liệu dưới dạng Columnar như Apache Parquet góp phần tăng hiệu năng truy xuất trên Spark lên rất nhiều lần. Bởi vì nó có thể tính toán và chỉ lấy ra 1 phần dữ liệu cần thiết (như 1 vài cột trên CSV), mà không cần phải đụng tới các phần khác của data row. Ngoài ra Parquet còn hỗ trợ flexible compression do đó tiết kiệm được rất nhiều không gian HDFS.","thumbnail":"https://2.bp.blogspot.com/-e_wBjtB6Fl0/V-ID3ys6F9I/AAAAAAAAd_k/jRxF8H344KM_ywgsxVfQAPy3GDXAd1_fQCK4B/s1600/parquet-logo.png"}}},{"node":{"fields":{"slug":"/2016/09/chay-apache-spark-voi-jupyter-notebook.html","categorySlug":"/category/data/"},"frontmatter":{"title":"Chạy Apache Spark với Jupyter Notebook","date":"2016-09-20T14:36:00.001+07:00","category":"Data","description":"IPython Notebook là một công cụ tiện lợi cho Python. Ta có thể Debug chương trình PySpark Line-by-line trên IPython Notebook một cách dễ dàng, tiết kiệm được nhiều thời gian.","thumbnail":"https://1.bp.blogspot.com/-IbzOyRw7mkM/V-Dm-cyXE9I/AAAAAAAAd-I/nGA92fFap4MM4uqKErB7g2H-t6T7CD1RQCLcB/s1600/Selection_006.png"}}},{"node":{"fields":{"slug":"/2016/09/pyspark-thieu-thu-vien-python-tren-worker.html","categorySlug":"/category/data/"},"frontmatter":{"title":"PySpark - Thiếu thư viện Python trên Worker","date":"2016-09-08T09:55:00.000+07:00","category":"Data","description":"Apache Spark chạy trên Cluster, với Java thì đơn giản. Với Python thì package python phải được cài trên từng Node của Worker. Nếu không bạn sẽ gặp phải lỗi thiếu thư viện.","thumbnail":null}}},{"node":{"fields":{"slug":"/2016/06/vntokenizer-tren-apache-spark.html","categorySlug":"/category/machine-learning/"},"frontmatter":{"title":"Chạy vnTokenizer trên môi trường Apache Spark","date":"2016-06-23T22:40:00.003+07:00","category":"Machine Learning","description":"vnTokenizer là công cụ chuyên dùng tách từ, gán nhãn từ loại cho tiếng Việt, của tác giả Lê Hồng Phương. vnTokenizer được viết bằng Java, có thể sử dụng như Tools Command Line hoặc Programming.","thumbnail":null}}},{"node":{"fields":{"slug":"/2015/12/apache-spark-on-docker.html","categorySlug":"/category/data/"},"frontmatter":{"title":"Apache Spark on Docker","date":"2015-12-12T00:48:00.000+07:00","category":"Data","description":"Docker and Spark are two technologies which are very hyped these days","thumbnail":"https://1.bp.blogspot.com/-KvlK1aCu4JA/VmsMOTCCySI/AAAAAAAALYo/H_kBQPB_dDE/s1600/KuDr42X_ITXghJhSInDZekNEF0jLt3NeVxtRye3tqco.png"}}},{"node":{"fields":{"slug":"/2015/12/map-reduce-va-bai-toan-wordcount.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"Bigdata - Map-Reduce và bài toán Word Count","date":"2015-12-02T21:58:00.001+07:00","category":"Data Engineer","description":"Map-Reduce là một giải pháp! Map-Reduce được phát minh bởi các kỹ sư Google để giải quyết bài toán xử lý một khối lượng dữ liệu cực lớn, vượt quá khả năng xử lý của một máy tính đơn có cấu hình khủng.","thumbnail":"https://3.bp.blogspot.com/-i_xNRnGm_pY/Vl8HH5-TM0I/AAAAAAAAKKU/K1W4w2i2f5E/s1600/big-data-cloud-e1383271750410-460x394.png"}}}]}},"pageContext":{"tag":"Apache Spark","currentPage":0,"postsLimit":8,"postsOffset":0,"prevPagePath":"/tag/apache-spark","nextPagePath":"/tag/apache-spark/page/1","hasPrevPage":false,"hasNextPage":true}},"staticQueryHashes":["251939775","2672868365","401334301"]}