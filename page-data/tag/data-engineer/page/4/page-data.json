{"componentChunkName":"component---src-templates-tag-template-tsx","path":"/tag/data-engineer/page/4","result":{"data":{"site":{"siteMetadata":{"title":"Tôi là Duyệt","subtitle":"Data Engineer @ Fossil. I blog about web development, machine learning, data engineering and more."}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/2016/12/vntokenizer-tren-pyspark.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"vnTokenizer trên PySpark","date":"2016-12-14T08:00:00.001+07:00","category":"Data Engineer","description":"Trong blog này mình sẽ custom lại vn.vitk để có thể chạy như một thư viện lập trình, sử dụng ngôn ngữ python (trên PySpark và Jupyter Notebook).","thumbnail":null}}},{"node":{"fields":{"slug":"/2016/11/r-tren-jupiter-notebook-ubuntu-1404.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"R trên Jupiter Notebook (Ubuntu 14.04 / 14.10 / 16.04)","date":"2016-11-22T23:53:00.000+07:00","category":"Data Engineer","description":"Jupyter Notebook là công cụ khá mạnh của lập trình viên Python và Data Science. Nếu dùng R, Jupyter cũng cho phép ta tích hợp R kernel vào Notebook một cách dễ dàng.","thumbnail":"https://4.bp.blogspot.com/-aQIMnwL9Gxc/WDR00IF9dqI/AAAAAAAAf9w/ZvplzJNUtI8vjWh2nF8_kVJZoYF3fHF9QCLcB/s1600/Screenshot%2Bfrom%2B2016-11-22%2B23-39-25.png"}}},{"node":{"fields":{"slug":"/2016/09/spark-convert-text-csv-to-parquet.html","categorySlug":"/category/data-engineer/"},"frontmatter":{"title":"Spark: Convert Text (CSV) to Parquet để tối ưu hóa Spark SQL và HDFS","date":"2016-09-21T10:54:00.000+07:00","category":"Data Engineer","description":"Lưu trữ dữ liệu dưới dạng Columnar như Apache Parquet góp phần tăng hiệu năng truy xuất trên Spark lên rất nhiều lần. Bởi vì nó có thể tính toán và chỉ lấy ra 1 phần dữ liệu cần thiết (như 1 vài cột trên CSV), mà không cần phải đụng tới các phần khác của data row. Ngoài ra Parquet còn hỗ trợ flexible compression do đó tiết kiệm được rất nhiều không gian HDFS.","thumbnail":"https://2.bp.blogspot.com/-e_wBjtB6Fl0/V-ID3ys6F9I/AAAAAAAAd_k/jRxF8H344KM_ywgsxVfQAPy3GDXAd1_fQCK4B/s1600/parquet-logo.png"}}},{"node":{"fields":{"slug":"/2016/09/chay-apache-spark-voi-jupyter-notebook.html","categorySlug":"/category/data/"},"frontmatter":{"title":"Chạy Apache Spark với Jupyter Notebook","date":"2016-09-20T14:36:00.001+07:00","category":"Data","description":"IPython Notebook là một công cụ tiện lợi cho Python. Ta có thể Debug chương trình PySpark Line-by-line trên IPython Notebook một cách dễ dàng, tiết kiệm được nhiều thời gian.","thumbnail":"https://1.bp.blogspot.com/-IbzOyRw7mkM/V-Dm-cyXE9I/AAAAAAAAd-I/nGA92fFap4MM4uqKErB7g2H-t6T7CD1RQCLcB/s1600/Selection_006.png"}}},{"node":{"fields":{"slug":"/2016/09/pyspark-thieu-thu-vien-python-tren-worker.html","categorySlug":"/category/data/"},"frontmatter":{"title":"PySpark - Thiếu thư viện Python trên Worker","date":"2016-09-08T09:55:00.000+07:00","category":"Data","description":"Apache Spark chạy trên Cluster, với Java thì đơn giản. Với Python thì package python phải được cài trên từng Node của Worker. Nếu không bạn sẽ gặp phải lỗi thiếu thư viện.","thumbnail":null}}}]}},"pageContext":{"tag":"Data Engineer","currentPage":4,"postsLimit":8,"postsOffset":32,"prevPagePath":"/tag/data-engineer/page/3","nextPagePath":"/tag/data-engineer/page/5","hasPrevPage":true,"hasNextPage":false}},"staticQueryHashes":["251939775","2672868365","401334301"]}