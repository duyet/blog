---
template: post
title: Running Apache Spark on Kubernetes at Fossil
date: '2021-02-20T00:00:00.000+07:00'
author: Van-Duyet Le
category: Data Engineer
tags:
  - Data Engineer
  - Apache Spark
  - Big Data
  - Fossil
thumbnail:
slug: /2021/03/running-apache-spark-on-kubernetes-at-fossil.html
draft: true
description: Running Apache at Fossil
fbCommentUrl: none
---

At Fossil, we processing billion of records by collecting, transforming and pushlishing data to many Data Warehouse and Dashboards though our Data Platform.
The Data Platform is a data-driven design with Lambda Architecture, including near-realtime and batch layer. The most of components are seprated to components deployed in Kubernetes, including Airflow on Kubernetes. Some of components are:
- API for Data Collector
- Extraction worker
- Transformation Worker
- Data Loading Worker to DWH
- Airflow on Kubernetes
- Spark and Hive on EMR for batch processing and backfill
- ...

Apache Spark and Airflow is the the tools that we choose for batching layer.
From the beginning, our team decided to use Spark on EMR for easy deployment, for a long time, it's show that there are many limitation on production scale.
In this post, I will show you how and why we have a big move from Spark on AWS EMR to Kubernetes.

# Apache Spark

Spark is an open-source, scalable, massively parallel, in-memory execution engine for analytics applications.
From the beginning, we have design to have many jobs running on Apache Spark and Hive on EMR. For a long running of times,
there are many disadvantaged, such as:
- Not easy to scale the node
- Cost
- ...

Since the whole system is a micro-services and event-driven design with many components running on Kubernetes.
We thinking about migrating all the jobs from EMR to Kubernetes.

There are many reasons to run Spark on Kubernetes instead of EMR:
- Cost savings, as we deployed Spark clusters using Amazon EMR, dealing with the complexity of provisioning and bootstrapping at the expense of a per-instance, per-second fee, and EMR operational costs, itâ€™s about $700-$800 for management, not including EC2 instance costs.
- Spark on YARN in EMR also comes with a high maintenance cost.
- Kubernetes provides a practical approach to isolated workloads, limits the use of resources, deploys on-demand, and scales as needed.

# Spark on Kubernetes v1

Since Spark version 2.3.x, Spark supported running clusters managed by Kubernetes. We can submit directly by using `spark-submit` on your command line,
what we need is update the `--master` to `k8s://<api_server_host>:<k8s-apiserver-port>`.
To launch Spark Pi in cluster mode, following this:

```bash
$ bin/spark-submit \
    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=<spark-image> \
    local:///path/to/examples.jar
```

The first architecture of Spark on Kubernetes like the following:

![Spark on Kubernetes v1 at Fossil](https://github.com/duyet/blog.duyet.net/blob/post/spark-at-fossil/content/media/2021/spark-on-k8s/spark-on-k8s-v1.png?raw=true)

We using Livy, which is a service enables to interaction with Spark Cluster through RESTful API. Many kind of DAG will be triggered by Airflow Scheduler, make a API trigger to Livy to submit, tracking and validate the results.


# Spark on Kubernetes v2

Because of the above disadvantaged and instability, the team decided to update the the version 2 of the architecture, after evaluating the Spark Operator by Google.


![Spark on Kubernetes v2 at Fossil](https://github.com/duyet/blog.duyet.net/blob/post/spark-at-fossil/content/media/2021/spark-on-k8s/spark-on-k8s-v2.png?raw=true)

In this architect, Data Engineer will
(1) generate Spark Jobs and commit to Git repo,
(2) Spark Submit worker will apply these Spark Jobs (ScheduledSparkApplication) to Kubernetes thought API,
(3) Spark Operator will specifying, running, and surfacing status of Spark applications, all spark app will running inside Kubernetes POD, logs will be stored at S3 bucket,
(4) Spark History Server will render theses logs for traceback by engineers,
(5) the Spark Jobs UI also helps to manage all artifacts, spark jobs status and simple validate the data.


## The Spark Operator

The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources (CRD) for specifying, running, and surfacing status of Spark applications.

For more information, check the Design, API Specification and detailed User Guide.

Spark Operator is installed at namespace `spark-jobs` via Helm Chart with `webhook` enabled. The following will install Spark Operator if you are using Helm:

```bash
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm install spark-operator \
  spark-operator/spark-operator \
  --namespace spark-jobs \
  --set sparkJobNamespace=spark-jobs \
  --set webhook.enable=true
```

Note that we need to custom the Spark Operator images to support `s3a://` scheme:

```Dockerfile
FROM gcr.io/spark-operator/spark-operator:v1beta2-1.2.1-3.0.0

# Add s3a connector
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar $SPARK_HOME/jars
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4.2/aws-java-sdk-1.7.4.2.jar $SPARK_HOME/jars
```

## Spark Submit Worker

This is the operator services to sync the `SparkApplication` and `ScheduledSparkApplication` artifacts from Git reposistory to the Kubernetes.

### The Spark Jobs UI

Spark Jobs UI or Spark Jobs Dashboard is interface to manage the artifacts that generated or customized by engineers, and submited to Kubernetes. This dashboard will including these basic features:
- Listing the YAML artifacts
- View the detailed of Spark Application YAML files
- View the status for each Scheduled Spark Application, like `scheduleStatus`, `lastRun`, `nextRun`, ...
- Basic validation for the output for each job interval.
- Generate and trigger the backfill.

I'm not sure if there components can be open source in the feature, but let see some screenshot:

### The Spark History Server

The Spark History Server is a monitoring tool that displays information about completed Spark applications. This information is pulled from the data that applications by default write to a directory on S3 bucket. Every SparkApplication including this configuration to push to Spark events to S3:

```yaml
spec:
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://spark-jobs-bucket/logs/"
```

The Spark History Server can be installed via [this Helm Chart](https://artifacthub.io/packages/helm/spot/spark-history-server), make sure the point the `logDirectory` to your bucket that Spark has send to.

```bash
helm repo add stable https://kubernetes-charts.storage.googleapis.com
helm install stable/spark-history-server \
    --namespace spark-jobs \
    --set enableS3=true \
    --set logDirectory=s3a://spark-jobs-bucket/logs/
```

# Spark Performance Tuning

There are many tuning about performance and costing that I've apply when running Spark on Kubernetes:
- Using Volcano Scheduler for Gang schedule
- Using Kryo serialization
- Ignoring Data Locality because of S3 data source.
- I/O for S3
- Tuning Java
- Enabled Dynamic Allocation and Dynamic Allocation Shuffle File Tracking
- ...
- Node spot instance for executor


# References
